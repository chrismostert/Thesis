{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring AcousticBrainz classifier stability\n",
    "\n",
    "AcousticBrainz has a large amount of classifier information available, and is used quite often as a source for psychological claims based on music like listening preferences over time, the influence of the seasons on our preferences, or claims that [pop music is getting sadder](http://www.bbc.com/culture/story/20190513-is-pop-music-really-getting-sadder-and-angrier).\n",
    "\n",
    "Low level features are known to be unstable (see reference in my notes), so the hypothesis is that these results from the AcousticBrainz classifiers are very much dependent on things like source quality. Furthermore, high-level features have additional problems like which emotion model do you use, and differences in interpretation, for example: how do you interpret a 'party mood'? Furthermore, the 'ground truth' that these models are trained on is also subjective. If scientific claims made using the results from such classifiers as a basis, then these claims might not be true if the classifiers are unreliable.\n",
    "\n",
    "Due to the crowdsourcing nature of AcousticBrainz, multiple submissions exist for the same recording, meaning that the classifier has been run multiple times over different submissions of the same recording. If these classifiers are accurate, then the results should remain fairly stable when minor variations in for example audio quality occur - a sad song should not become happy if the quality is higher, for example. \n",
    "\n",
    "Thus, the first question to answer is: **How stable are the classifiers included in AcousticBrainz** and a second question that arises is **Which classifiers are relatively stable, and which classifiers are relatively unstable?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import all required packages and load in the acousticbrainz dataset which was generated by running the scripts in ```acousticbrainz_data_generation```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94684c03f7e54f5c929d6ea39323059b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\Miniconda3\\envs\\thesis\\lib\\site-packages\\tqdm\\std.py:651: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File C:\\Users\\Chris\\Documents\\Thesis\\datasets\\acousticbrainz.h5 does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d16a8416253b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Load in the acousticbrainz dataset into the variable 'acousticbrainz'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0macousticbrainz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'datasets'\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'acousticbrainz.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\thesis\\lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36mread_hdf\u001b[1;34m(path_or_buf, key, mode, **kwargs)\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m             raise FileNotFoundError(\n\u001b[1;32m--> 381\u001b[1;33m                 \u001b[1;34m\"File {path} does not exist\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m             )\n\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File C:\\Users\\Chris\\Documents\\Thesis\\datasets\\acousticbrainz.h5 does not exist"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Use seaborn style defaults and set the default figure size\n",
    "sns.set(rc={'figure.figsize':(15, 15)})\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm().pandas()\n",
    "\n",
    "\n",
    "# Load in the acousticbrainz dataset into the variable 'acousticbrainz'\n",
    "acousticbrainz = pd.read_hdf(Path.cwd() / 'datasets' / 'acousticbrainz.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is indexed into two levels. The first level is the MBID and the second level is the submission id. Cell values are the label probabilities as given back by the classifier. The dataframe looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acousticbrainz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some recordings have many submissions, like `Bohemian Rhapsody`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acousticbrainz.loc['b1a9c0e9-d987-4042-ae91-78d6a3267d69']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe contains the following classifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acousticbrainz.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier variance\n",
    "There are multiple ways to look at this, we can either see how stable the *probabilities* are, i.e. how stable is the certainty of the classifier in the label being a specific value or we can see how stable the *labels* are, i.e. for all submissions are the labels the same or do they flip?\n",
    "\n",
    "We'll begin with the first case.\n",
    "\n",
    "We are interested in the probability values for a given label for the independent variable mbid. Some mbids only have one submission. These do not give us any information about the variance and should be filtered out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filt = acousticbrainz.groupby(level=0).size() > 1\n",
    "acousticbrainz = acousticbrainz[filt[acousticbrainz.index.get_level_values(level=0)].values]\n",
    "\n",
    "acousticbrainz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have many populations with relatively small sizes, and a few populations with a bit more (~30), however these are not sample sizes large enough to give us a good estimate of the classifier variance on the same songs.\n",
    "\n",
    "However, we can calculate the variance for each individual population (with the probabilities in each population indexed $j=0,...,j=n_{i}-1$\n",
    "$$s_{i}^{2} = \\frac{1}{n_{i}-1} \\sum_{j=0}^{j=n_{i}-1}(y_{j} - \\bar{y_i})^2$$\n",
    "\n",
    "And then compute the pooled variance for each classifier by taking the weighted average for all $k$ populations indexed $k=0,...,k-1$ \n",
    "$$s_{p}^{2} = \\frac{\\sum_{i=0}^{k-1}(n_{i}-1)s_{i}^{2}}{\\sum_{i=0}^{k-1}(n_{i}-1)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = acousticbrainz.groupby(level=0).var()\n",
    "samplesizes = acousticbrainz.groupby(level=0).size()\n",
    "\n",
    "pooledvariance = (variances.mul(samplesizes-1, axis=0).sum()) / (samplesizes.sum() - samplesizes.count())\n",
    "\n",
    "print(pooledvariance.sort_values().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete label variance\n",
    "Some classifiers, like `voice_instrumental`, `danceability` and `timbre` have relatively high variance. In the context of these calculations, that means that these classifiers can be seen as being somewhat **unreliable** or **uncertain**, since the probability values for the labels for the same recordings vary a lot. Other classifiers, like `genre_dortmund` seem to be very **reliable**, with very low variance. However, are these classifiers more reliable or simply more biased, predicting the same label every time thus lowering the variance?\n",
    "\n",
    "## 'Biasedness' of the classifiers\n",
    "To quantify this, we first need to transform the probabilities to hard labels. For this we select the most probable label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grouped = acousticbrainz.groupby(axis=1, level=0).idxmax(axis=1).applymap(lambda x: x[1])\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for every column we can calculate the **entropy**, if a classifier always or nearly always produces the same result, then the informational value and thus the entropy will be low or even zero.\n",
    "$$S_n(p)=-\\sum_{i}p_{i}\\log_2 p_{i}$$\n",
    "\n",
    "With $$S_{max} = \\log_2 n$$\n",
    "\n",
    "To compare the entropies of the different classifiers, normalize them by their maximum values so every entropy falls in $[0,1]$:\n",
    "\n",
    "$$S=-\\sum_{i}\\frac{p_i \\log_2 p_i}{log_2 n}$$\n",
    "\n",
    "Which is equivalent to\n",
    "\n",
    "$$S=-\\sum_{i}p_i \\log_n p_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import entropy\n",
    "\n",
    "probabilities = pd.DataFrame()\n",
    "for c in grouped.columns:\n",
    "    probabilities = probabilities.append(grouped[c].value_counts(normalize=True)).fillna(value=0)\n",
    "    \n",
    "normalizers = acousticbrainz.groupby(level = 0, axis = 1).size()\n",
    "\n",
    "norm_entropy = probabilities.apply(lambda row: entropy(row, base=normalizers[row.name]), axis=1)\n",
    "norm_entropy.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between biasedness and variance\n",
    "Now we are interested in the relation between the entropy (i.e. roughly how biased the classifier is) and the variance of the probabilities of that classifier.\n",
    "\n",
    "Note that for classifiers with only two labels, the variances of those two labels are the same (values are either one or the other), however for multiple labels the variances differ. For comparisons sake we take the average variance for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_variance = pooledvariance.mean(level=0)\n",
    "avg_variance.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, ideally a classifier has low bias (high entropy) and low variance when running on different quality levels of the same recording (low pooled variance, high 'reliability'). We are interested in the relation between these two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toplot = pd.DataFrame(columns=['Normalized entropy', 'Pooled variance'])\n",
    "toplot['Normalized entropy'] = norm_entropy\n",
    "toplot['Pooled variance'] = avg_variance\n",
    "\n",
    "display(toplot)\n",
    "# plot = toplot.plot(kind='scatter', x='Pooled variance', y='Normalized entropy')\n",
    "\n",
    "p1 = sns.regplot('Normalized entropy', 'Pooled variance', toplot, fit_reg=False)\n",
    "for index, val in toplot.iterrows():\n",
    "    p1.text(val['Normalized entropy'] + 0.005, val['Pooled variance'] + 0.0005, index, horizontalalignment='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance over labels instead of over probabilities\n",
    "It can be argued that variance in the probability distribution over the labels is not harmful, as long as the labels themselves remain stable. It is more harmful when a classifier 'flips' the label. To calculate this variance, we take the same approach as with the probabilities by pooling the different populations, however now we look at the discrete labels.\n",
    "\n",
    "For this we can, again use the normalized entropy:\n",
    "$$S=-\\sum_{i}\\frac{p_i \\log_2 p_i}{log_2 n}$$\n",
    "However, now in the best case the entropy for a population is 0 (the label does not flip), and higher entropy means more flips and thus less reliability\n",
    "\n",
    "\n",
    "We first calculate the label probabilities per recording:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_probabilities = grouped.stack().groupby(level=[0,2]).value_counts(normalize=True).unstack().fillna(value=0)\n",
    "population_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pool the entropy much in the way we pooled the variances by taking the weighted average:\n",
    "$$S_w=\\frac{\\sum_{i=0}^{k-1}n_i S_n}{\\sum_{i=0}^{k-1}n_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_entropies = population_probabilities.progress_apply(lambda row: entropy(row, base=normalizers[row.name[1]]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pooled_entropy = (pop_entropies.unstack().mul(samplesizes, axis=0)).sum() / samplesizes.sum()\n",
    "pooled_entropy.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these values it holds, the higher the pooled entropy, the more the discrete label flips. Thus, lower values are more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "toplot = pd.DataFrame(columns=['Normalized entropy', 'Label variance (pooled entropy)'])\n",
    "toplot['Normalized entropy'] = norm_entropy\n",
    "toplot['Label variance (pooled entropy)'] = pooled_entropy\n",
    "\n",
    "display(toplot)\n",
    "p2 = sns.regplot('Normalized entropy', 'Label variance (pooled entropy)', toplot, fit_reg=False)\n",
    "for index, val in toplot.iterrows():\n",
    "    p2.text(val['Normalized entropy'] + 0.005, val['Label variance (pooled entropy)'] + 0.0005, index, horizontalalignment='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier correlations\n",
    "Another way to look at if the classifiers present in acousticbrainz do report what they intent to report in a *consistent* way is to check correlations between the different classifiers.\n",
    "\n",
    "The reasoning behind this is as follows: we know some correlations from experience and/or psychology, for example if a piece of music is happy, then it is probably not sad (and thus we would expect a negative correlation between `mood_happy` and `mood_sad`) or we would expect that `mood_party` and `dancability` are slightly correlated, since some, but not all parties involve dancing.\n",
    "\n",
    "If classifiers do not show the correlations we expect, then either:\n",
    "- The hypothesis of the correlation is wrong\n",
    "- The classifier does not (or does not fully) model the expected feature in the music correctly\n",
    "\n",
    "Thus, if we make a list of likely correlation hypotheses, then we can use the correlation between the classifiers as another metric for the reliability of the classifiers.\n",
    "\n",
    "### Correlation hypotheses\n",
    "#### Genre classification\n",
    "- `genre_dortmund, blues` - `genre_tzanetakis, blu`: strong positive correlation\n",
    "- `genre_dortmund, electronic` - `genre_tzanetakis, hip`: moderate positive correlation\n",
    "- `genre_dortmund, folkcountry` - `genre_tzanetakis, cou`: positive correlation\n",
    "- `genre_dortmund, jazz` - `genre_tzanetakis, jaz`: strong positive correlation\n",
    "- `genre_dortmund, pop` - `genre_tzanetakis, pop`: strong positive correlation\n",
    "- `genre_dortmund, raphiphop` - `genre_tzanetakis, hip`: positive correlation\n",
    "- `genre_dortmund, rock` - `genre_tzanetakis, roc`: strong positive correlation\n",
    "- `genre_dortmund, blues` - `genre_rosamerica, rhy`: positive correlation\n",
    "- `genre_dortmund, electronic` - `genre_rosamerica, dan`: moderate positive correlation\n",
    "- `genre_dortmund, jazz` - `genre_rosamerica, jaz`: strong positive correlation\n",
    "- `genre_dortmund, pop` - `genre_rosamerica, pop`: strong positive correlation\n",
    "- `genre_dortmund, raphiphop` - `genre_rosamerica, hip`: positive correlation\n",
    "- `genre_dortmund, rock` - `genre_rosamerica, roc`: strong positive correlation\n",
    "- `genre_rosamerica, cla` - `genre_tzanetakis, cla`: strong positive correlation\n",
    "- `genre_rosamerica, hip` - `genre_tzanetakis, hip`: strong positive correlation\n",
    "- `genre_rosamerica, jaz` - `genre_tzanetakis, jaz`: strong positive correlation\n",
    "- `genre_rosamerica, pop` - `genre_tzanetakis, pop`: strong positive correlation\n",
    "- `genre_rosamerica, rhy` - `genre_tzanetakis, blu`: positive correlation\n",
    "- `genre_rosamerica, roc` - `genre_tzanetakis, roc`: strong positive correlation\n",
    "#### Mirex clusters\n",
    "- `moods_mirex, cluster2` - `mood_happy, happy` : positive correlation\n",
    "- `moods_mirex, cluster2` - `mood_sad, not_sad` : positive correlation\n",
    "- `moods_mirex, cluster3` - `mood_happy, not_happy` : positive correlation\n",
    "- `moods_mirex, cluster3` - `mood_sad, sad` : positive correlation\n",
    "- `moods_mirex, cluster5` - `mood_aggressive, aggressive` : positive correlation\n",
    "- `moods_mirex, cluster5` - `mood_relaxed, not_relaxed`: positive correlation\n",
    "\n",
    "#### Other correlations\n",
    "- `danceability, danceable` - `mood_party, party`: positive correlation\n",
    "- `danceability, danceable` - `mood_relaxed, not_relaxed`: moderate positive correlation\n",
    "- `danceability, danceable` - `genre_rosamerica, dan`: positive correlation\n",
    "- `danceability, danceable` - `genre_tzanetakis, dis`: positive correlation\n",
    "- `mood_acoustic, acoustic` - `mood_electronic, not_electronic`: positive correlation\n",
    "- `mood_aggressive, aggressive` - `mood_relaxed, not_relaxed`: positive correlation\n",
    "- `mood_electronic, electronic` - `genre_dortmund, electronic`: strong positive correlation\n",
    "- `mood_happy, happy` - `mood_sad, not_sad`: positive correlation\n",
    "- `mood_happy, happy` - `mood_party, party`: positive correlation\n",
    "\n",
    "\n",
    "Now, let's calculate these correlations from the sample data and see if they match the hypotheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acousticbrainz = pd.read_hdf(Path.cwd() / 'datasets' / 'acousticbrainz.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
